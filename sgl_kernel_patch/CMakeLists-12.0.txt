cmake_minimum_required(VERSION 3.26 FATAL_ERROR)
project(sgl-kernel LANGUAGES CXX CUDA)

# CMake
#cmake_policy(SET CMP0169 OLD)
#cmake_policy(SET CMP0177 NEW)
include(${CMAKE_CURRENT_LIST_DIR}/cmake/utils.cmake)
set(CMAKE_COLOR_DIAGNOSTICS ON)
set(CMAKE_VERBOSE_MAKEFILE ON CACHE BOOL "ON")
set(CMAKE_POSITION_INDEPENDENT_CODE ON)
set(CMAKE_SHARED_LIBRARY_PREFIX "")

# Python
find_package(Python COMPONENTS Interpreter Development.Module ${SKBUILD_SABI_COMPONENT} REQUIRED)

# CXX
set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -O3")

# CUDA
enable_language(CUDA)
find_package(CUDAToolkit REQUIRED)
set_property(GLOBAL PROPERTY CUDA_SEPARABLE_COMPILATION ON)

message(STATUS "Detected CUDA_VERSION=${CUDA_VERSION}")
if ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "13.0")
    message("CUDA_VERSION ${CUDA_VERSION} >= 13.0")
elseif ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "12.8")
    message("CUDA_VERSION ${CUDA_VERSION} >= 12.8")
elseif ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "12.4")
    message("CUDA_VERSION ${CUDA_VERSION} >= 12.4")
elseif ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "12.1")
    message("CUDA_VERSION ${CUDA_VERSION} >= 12.1")
elseif ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "11.8")
    message("CUDA_VERSION ${CUDA_VERSION} >= 11.8")
endif()

# Torch
find_package(Torch REQUIRED)
# clean Torch Flag
clear_cuda_arches(CMAKE_FLAG)

include(FetchContent)

# cutlass
FetchContent_Declare(
    repo-cutlass
    GIT_REPOSITORY https://github.com/NVIDIA/cutlass
    GIT_TAG        57e3cfb47a2d9e0d46eb6335c3dc411498efa198
    GIT_SHALLOW    OFF
)
FetchContent_Populate(repo-cutlass)

#!# DeepGEMM
#!FetchContent_Declare(
#!    repo-deepgemm
#!    GIT_REPOSITORY https://github.com/sgl-project/DeepGEMM
#!    GIT_TAG        4d23df0a07b057fbb4a44ff8666e528a600feb5e
#!    GIT_SHALLOW    OFF
#!)
#!FetchContent_Populate(repo-deepgemm)
#!
#!FetchContent_Declare(
#!    repo-fmt
#!    GIT_REPOSITORY https://github.com/fmtlib/fmt
#!    GIT_TAG        553ec11ec06fbe0beebfbb45f9dc3c9eabd83d28
#!    GIT_SHALLOW    OFF
#!)
#!FetchContent_Populate(repo-fmt)
#!
#!# Triton
#!FetchContent_Declare(
#!    repo-triton
#!    GIT_REPOSITORY "https://github.com/triton-lang/triton"
#!    GIT_TAG        8f9f695ea8fde23a0c7c88e4ab256634ca27789f
#!    GIT_SHALLOW    OFF
#!)
#!FetchContent_Populate(repo-triton)

# flashinfer
set(FLASHINFER_DIR "${CMAKE_SOURCE_DIR}/flashinfer" CACHE PATH "FlashInfer checkout")
if(NOT EXISTS "${FLASHINFER_DIR}/csrc")
    message(FATAL_ERROR "FlashInfer sources missing at ${FLASHINFER_DIR}")
endif()

# Resolve apache-tvm-ffi headers (needed to build FlashInfer bindings here).
set(TVM_INCLUDE_DIRS "")
execute_process(
    COMMAND ${Python_EXECUTABLE} -c "import json; import tvm_ffi.libinfo as L; print(json.dumps([L.find_include_path(), L.find_dlpack_include_path()]))"
    OUTPUT_VARIABLE TVM_INCLUDE_JSON
    RESULT_VARIABLE TVM_INCLUDE_RESULT
    OUTPUT_STRIP_TRAILING_WHITESPACE
)
if (TVM_INCLUDE_RESULT EQUAL 0 AND NOT TVM_INCLUDE_JSON STREQUAL "")
    string(JSON TVM_INCLUDE_DIR_0 GET "${TVM_INCLUDE_JSON}" 0)
    string(JSON TVM_INCLUDE_DIR_1 GET "${TVM_INCLUDE_JSON}" 1)
    list(APPEND TVM_INCLUDE_DIRS "${TVM_INCLUDE_DIR_0}" "${TVM_INCLUDE_DIR_1}")
    list(REMOVE_DUPLICATES TVM_INCLUDE_DIRS)
else()
    message(FATAL_ERROR "Failed to locate apache-tvm-ffi headers. Ensure apache-tvm-ffi is installed in the active Python environment.")
endif()

#@# flash-attention
#@FetchContent_Declare(
#@    repo-flash-attention
#@    GIT_REPOSITORY https://github.com/sgl-project/sgl-attn
#@    GIT_TAG        sgl-kernel
#@    GIT_SHALLOW    OFF
#@)
#@FetchContent_Populate(repo-flash-attention)
#@
#@# flash-attention origin
#@FetchContent_Declare(
#@    repo-flash-attention-origin
#@    GIT_REPOSITORY https://github.com/Dao-AILab/flash-attention.git
#@    GIT_TAG        203b9b3dba39d5d08dffb49c09aa622984dff07d
#@    GIT_SHALLOW    OFF
#@)
#@FetchContent_Populate(repo-flash-attention-origin)
#@
#@# mscclpp
#@FetchContent_Declare(
#@    repo-mscclpp
#@    GIT_REPOSITORY https://github.com/microsoft/mscclpp.git
#@    GIT_TAG        51eca89d20f0cfb3764ccd764338d7b22cd486a6
#@    GIT_SHALLOW    OFF
#@)
#@FetchContent_Populate(repo-mscclpp)

#% # ccache option
#% option(ENABLE_CCACHE "Whether to use ccache" ON)
#% find_program(CCACHE_FOUND ccache)
#% if(CCACHE_FOUND AND ENABLE_CCACHE AND DEFINED ENV{CCACHE_DIR})
#%     message(STATUS "Building with CCACHE enabled")
#%     set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE "ccache")
#%     set_property(GLOBAL PROPERTY RULE_LAUNCH_LINK "ccache")
#% endif()

include_directories(
    ${PROJECT_SOURCE_DIR}/include
    ${PROJECT_SOURCE_DIR}/csrc
    ${repo-cutlass_SOURCE_DIR}/include
    ${repo-cutlass_SOURCE_DIR}/tools/util/include
    ${FLASHINFER_DIR}/include
    ${FLASHINFER_DIR}/csrc
    ${TVM_INCLUDE_DIRS}
    #${repo-mscclpp_SOURCE_DIR}/include
)

set(SGL_KERNEL_CUDA_FLAGS
    "-DNDEBUG"
    "-DOPERATOR_NAMESPACE=sgl-kernel"
    "-O3"
    "-Xcompiler"
    "-fPIC"
    "-gencode=arch=compute_90,code=sm_90"
    "-std=c++17"
    "-DFLASHINFER_ENABLE_F16"
    "-DCUTE_USE_PACKED_TUPLE=1"
    "-DCUTLASS_ENABLE_TENSOR_CORE_MMA=1"
    "-DCUTLASS_VERSIONS_GENERATED"
    "-DCUTLASS_TEST_LEVEL=0"
    "-DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1"
    "-DCUTLASS_DEBUG_TRACE_LEVEL=0"
    "--expt-relaxed-constexpr"
    "--expt-extended-lambda"

    # Supress warnings
    "-Xcompiler=-Wno-clang-format-violations"
    "-Xcompiler=-Wno-conversion"
    "-Xcompiler=-Wno-deprecated-declarations"
    "-Xcompiler=-Wno-terminate"
    "-Xcompiler=-Wfatal-errors"
    "-Xcompiler=-ftemplate-backtrace-limit=1"
    "-Xcudafe=--diag_suppress=177"   # variable was declared but never referenced
    "-Xcudafe=--diag_suppress=2361"  # invalid narrowing conversion from "char" to "signed char"

    # uncomment to debug
    # "--ptxas-options=-v"
    # "--ptxas-options=--verbose,--register-usage-level=10,--warn-on-local-memory-usage"
)

set(SGL_KERNEL_COMPILE_THREADS 8 CACHE STRING "Set compilation threads, default 32")

list(APPEND SGL_KERNEL_CUDA_FLAGS
    "--threads=${SGL_KERNEL_COMPILE_THREADS}"
)

option(SGL_KERNEL_ENABLE_BF16             "Enable BF16"             ON)
option(SGL_KERNEL_ENABLE_FP8              "Enable FP8"              OFF)
option(SGL_KERNEL_ENABLE_FP4              "Enable FP4"              OFF)
option(SGL_KERNEL_ENABLE_FA3              "Enable FA3"              OFF)
option(SGL_KERNEL_ENABLE_SM90A            "Enable SM90A"            OFF)
option(SGL_KERNEL_ENABLE_SM100A           "Enable SM100A"           OFF)

if (SGL_KERNEL_ENABLE_BF16)
    list(APPEND SGL_KERNEL_CUDA_FLAGS
        "-DFLASHINFER_ENABLE_BF16"
    )
endif()

if (SGL_KERNEL_ENABLE_FP8)
    list(APPEND SGL_KERNEL_CUDA_FLAGS
        "-DFLASHINFER_ENABLE_FP8"
        "-DFLASHINFER_ENABLE_FP8_E4M3"
        "-DFLASHINFER_ENABLE_FP8_E5M2"
    )
endif()

list(APPEND SGL_KERNEL_CUDA_FLAGS
    #"-gencode=arch=compute_89,code=sm_89"
    "-gencode=arch=compute_120a,code=sm_120a"
    #"-gencode=arch=compute_90a,code=sm_90a"
)

if ("${CUDA_VERSION}" VERSION_GREATER_EQUAL "12.8" OR SGL_KERNEL_ENABLE_FP4)
    list(APPEND SGL_KERNEL_CUDA_FLAGS
        "-DENABLE_NVFP4=1"
    )
endif()

set(SOURCES
    #"csrc/allreduce/custom_all_reduce.cu"
    #"csrc/allreduce/mscclpp_allreduce.cu"
    "csrc/attention/cascade.cu"
    "csrc/attention/cutlass_mla_kernel.cu"
    "csrc/attention/lightning_attention_decode_kernel.cu"
    "csrc/attention/merge_attn_states.cu"
    #"csrc/attention/vertical_slash_index.cu"
    "csrc/elementwise/activation.cu"
    "csrc/elementwise/cast.cu"
    "csrc/elementwise/copy.cu"
    #"csrc/elementwise/concat_mla.cu"
    "csrc/elementwise/fused_add_rms_norm_kernel.cu"
    "csrc/elementwise/rope.cu"
    "csrc/elementwise/topk.cu"
    "csrc/common_extension.cc"

    #?    "csrc/gemm/awq_kernel.cu"
    #?    "csrc/gemm/bmm_fp8.cu"
    #?    "csrc/gemm/dsv3_fused_a_gemm.cu"
    #?    "csrc/gemm/dsv3_router_gemm_bf16_out.cu"
    #?    "csrc/gemm/dsv3_router_gemm_entry.cu"
    #?    "csrc/gemm/dsv3_router_gemm_float_out.cu"
    #?    "csrc/gemm/fp8_blockwise_gemm_kernel.cu"
    #?    "csrc/gemm/fp8_gemm_kernel.cu"
    #?    "csrc/gemm/int8_gemm_kernel.cu"
    #?    "csrc/gemm/nvfp4_expert_quant.cu"
    #?    "csrc/gemm/nvfp4_quant_entry.cu"
    #?    "csrc/gemm/nvfp4_quant_kernels.cu"
    #?    "csrc/gemm/nvfp4_scaled_mm_entry.cu"
    #?    "csrc/gemm/nvfp4_scaled_mm_kernels.cu"
    #?    "csrc/gemm/per_tensor_quant_fp8.cu"
    #?    "csrc/gemm/per_token_group_quant_8bit.cu"
    #?    "csrc/gemm/per_token_quant_fp8.cu"
    #?    "csrc/gemm/qserve_w4a8_per_chn_gemm.cu"
    #?    "csrc/gemm/qserve_w4a8_per_group_gemm.cu"
    #?    "csrc/gemm/marlin/gptq_marlin.cu"
    #?    "csrc/gemm/marlin/gptq_marlin_repack.cu"
    #?    "csrc/gemm/marlin/awq_marlin_repack.cu"
    #?    "csrc/gemm/gptq/gptq_kernel.cu"
    #?
    #?    "csrc/grammar/apply_token_bitmask_inplace_cuda.cu"
    #?
    #?    "csrc/mamba/causal_conv1d.cu"

    #$ "csrc/moe/cutlass_moe/w4a8/scaled_mm_entry.cu"
    #$ "csrc/moe/cutlass_moe/w4a8/w4a8_moe_data.cu"
    #$ "csrc/moe/cutlass_moe/w4a8/w4a8_grouped_mm_c3x.cu"
    #$ "csrc/moe/marlin_moe_wna16/ops.cu"
    #$ "csrc/moe/moe_align_kernel.cu"
    #$ "csrc/moe/moe_fused_gate.cu"
    #$ "csrc/moe/moe_sum_reduce.cu"
    #$ "csrc/moe/moe_topk_softmax_kernels.cu"
    #$ "csrc/moe/nvfp4_blockwise_moe.cu"
    #$ "csrc/moe/fp8_blockwise_moe_kernel.cu"
    #$ "csrc/moe/prepare_moe_input.cu"

    "csrc/memory/store.cu"
    "csrc/kvcacheio/transfer.cu"

    "csrc/speculative/eagle_utils.cu"
    "csrc/speculative/ngram_utils.cu"
    "csrc/speculative/packbit.cu"
    "csrc/speculative/speculative_sampling.cu"

    "${FLASHINFER_DIR}/csrc/norm.cu"
    "${FLASHINFER_DIR}/csrc/renorm.cu"
    "${FLASHINFER_DIR}/csrc/sampling.cu"

    #. "${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src/flash_fwd_sparse_hdim128_bf16_causal_sm80.cu"
    #. "${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src/flash_fwd_sparse_hdim128_bf16_sm80.cu"
    #. "${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src/flash_fwd_sparse_hdim128_fp16_causal_sm80.cu"
    #. "${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src/flash_fwd_sparse_hdim128_fp16_sm80.cu"
    #. "${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/flash_sparse_api.cpp"
)

#? # Build SM90 library with fast math optimization (same namespace, different directory)
#? Python_add_library(common_ops_sm90_build MODULE USE_SABI ${SKBUILD_SABI_VERSION} WITH_SOABI ${SOURCES})
#? 
#? target_compile_definitions(common_ops_sm90_build PRIVATE
#?     USE_FAST_MATH=1
#? )
#? target_compile_options(common_ops_sm90_build PRIVATE
#?     $<$<COMPILE_LANGUAGE:CUDA>:${SGL_KERNEL_CUDA_FLAGS} -use_fast_math>
#? )
#? target_include_directories(common_ops_sm90_build PRIVATE
#?     ${PROJECT_SOURCE_DIR}/csrc
#?     ${repo-cutlass_SOURCE_DIR}/examples/77_blackwell_fmha
#?     ${repo-cutlass_SOURCE_DIR}/examples/common
#?     ${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src
#? )
#? # Set output name and separate build directory to avoid conflicts
#? set_target_properties(common_ops_sm90_build PROPERTIES
#?     OUTPUT_NAME "common_ops"
#?     LIBRARY_OUTPUT_DIRECTORY "${CMAKE_CURRENT_BINARY_DIR}/sm90"
#? )
#? 
#? target_link_libraries(common_ops_sm90_build PRIVATE ${TORCH_LIBRARIES} c10 cuda cublas cublasLt)
#? install(TARGETS common_ops_sm90_build LIBRARY DESTINATION sgl_kernel/sm90)

# Build SM100+ library with precise math (same namespace, different directory)
Python_add_library(common_ops_sm100_build MODULE USE_SABI ${SKBUILD_SABI_VERSION} WITH_SOABI ${SOURCES})

target_compile_definitions(common_ops_sm100_build PRIVATE
    USE_FAST_MATH=0
)
target_compile_options(common_ops_sm100_build PRIVATE
    $<$<COMPILE_LANGUAGE:CUDA>:${SGL_KERNEL_CUDA_FLAGS}>
)
target_include_directories(common_ops_sm100_build PRIVATE
    ${PROJECT_SOURCE_DIR}/csrc
    ${repo-cutlass_SOURCE_DIR}/examples/77_blackwell_fmha
    ${repo-cutlass_SOURCE_DIR}/examples/common
    #${repo-flash-attention_SOURCE_DIR}/csrc/flash_attn/src
)
# Set output name and separate build directory to avoid conflicts
set_target_properties(common_ops_sm100_build PROPERTIES
    OUTPUT_NAME "common_ops"
    LIBRARY_OUTPUT_DIRECTORY "${CMAKE_CURRENT_BINARY_DIR}/sm100"
)

find_package(Python3 COMPONENTS Interpreter REQUIRED)
execute_process(
    COMMAND ${Python3_EXECUTABLE} -c "import torch; print(int(torch._C._GLIBCXX_USE_CXX11_ABI))"
    OUTPUT_VARIABLE TORCH_CXX11_ABI
    OUTPUT_STRIP_TRAILING_WHITESPACE
)
if(TORCH_CXX11_ABI STREQUAL "0")
    message(STATUS "Using old C++ ABI (-D_GLIBCXX_USE_CXX11_ABI=0)")
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=0")
    set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=0")
else()
    message(STATUS "Using new C++11 ABI (-D_GLIBCXX_USE_CXX11_ABI=1)")
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=1")
    set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=1")
endif()


target_link_libraries(common_ops_sm100_build PRIVATE ${TORCH_LIBRARIES} c10 cuda cublas cublasLt)
install(TARGETS common_ops_sm100_build LIBRARY DESTINATION sgl_kernel/sm100)
