[DEFAULT]
debug = False
seed = 0
output_dir = 'output'
run_name = 'temp_run'
git_sha1 = 'unknown'
git_diff = ''

[training]
project = 'eagle4'
resume_from_checkpoint = False
resume_wandb_runid = None
max_length = 2048
filter_out_shorts = False
deepspeed = None
use_default_num_items_getter = True
use_eagle_pipeline = False
model_init_ckpt = None

overwrite_output_dir = True
save_strategy = "steps"
save_steps = 500
save_total_limit = 2
optim = "adamw_torch_fused"
# same as Hydra++
num_train_epochs = 10
max_steps = -1
per_device_train_batch_size = 2
gradient_accumulation_steps = 3
average_tokens_across_devices = False
warmup_steps = 2000
#warmup_steps = 300
lr_scheduler_type = "constant_with_warmup"
learning_rate = 3e-5
ddp_find_unused_parameters = False
max_grad_norm = 0.5
adam_beta1 = 0.9
adam_beta2 = 0.95
logging_steps = 25
logging_first_step = True
bf16 = True
tf32 = False
report_to = "none"
#eval_strategy = "no"
eval_strategy = "steps"
per_device_eval_batch_size = 1
eval_steps = 100
# Always set dataloader_drop_last to avoid empty batch for some node when world > 1.
dataloader_drop_last = True
# in order for dataloader_drop_last to be effective, avoid using streaming load_dataset:
# See transformers/trainer.py:get_train_dataloader() and
# https://huggingface.co/docs/datasets/en/about_mapstyle_vs_iterable
ddp_backend = "gloo"
#ddp_backend = "nccl"

[dataset]
#manual_sample_ids = ['v4PzAY8_0', 'oM7QCY2_0', 'efVCaLN_0']
manual_sample_ids = []
read_eagle_format = False
max_read_items = None
path = "output/datasets/default"
eval_path = None

[dataset_generation]
save_every = 1000
batch_size = 1
#debug_target = v4PzAY8_0
debug_target = None
ds_prefix = 'ds_'
max_length = 2048
sharegpt_path = 'Aeala/ShareGPT_Vicuna_unfiltered'

[modeling]
tokenizer_init = ""
dtype = 'torch.bfloat16'
set_base_config_path_to_zero = False
model_path = None

[inference]
max_new_tokens = 512
mode = 'speculative'
draft_tree_shape = 'mc_sim_7b_64'
timer = False
alpha_stats = False
interactive = False
draft_growing = False
max_draft_growing_depth = 100
dynamic_draft = True
dynamic_draft_max_depth = 5
dynamic_draft_top_k = 10
# avg 27 for static draft:
dynamic_draft_all_top_k = 59

[modeling.@llama2_7b]
tokenizer_path = "meta-llama/Llama-2-7b-chat-hf"
chat_template = "models/speculative_llama/chat_template.jinja2"
tokenizer_add_tokens = "dict(pad_token=tokenizer.eos_token)"

model_path = "meta-llama/Llama-2-7b-chat-hf"

[modeling.@llama2_7b_base_and_llama2_7b_drafter_using_eagle2]
tokenizer_path = "meta-llama/Llama-2-7b-chat-hf"
chat_template = "models/speculative_llama/chat_template.jinja2"
tokenizer_add_tokens = "dict(pad_token=tokenizer.eos_token)"

init_base_model = ('SpeculativeLlamaForCausalLM', 'meta-llama/Llama-2-7b-chat-hf')
init_draft_config = ('LlamaDrafter', 'meta-llama/Llama-2-7b-chat-hf')
init_speculative_algorithm = ('EagleV2', 'dict(draft_layers=1)')
set_base_config_path_to_zero = 'num_hidden_layers'

[modeling.@llama2_7b_base_and_qwen3_moe_30A3B_drafter_using_eagle2]
tokenizer_path = "meta-llama/Llama-2-7b-chat-hf"
chat_template = "models/speculative_qwen3_moe/chat_template.jinja2"
tokenizer_add_tokens = "dict(pad_token=tokenizer.eos_token)"

init_base_model = ('SpeculativeLlamaForCausalLM', 'meta-llama/Llama-2-7b-chat-hf')
init_draft_config = ('Qwen3MoeDrafter', 'Qwen/Qwen3-30B-A3B-Instruct-2507')
init_speculative_algorithm = ('EagleV2', 'dict(draft_layers=1)')
set_base_config_path_to_zero = 'num_hidden_layers'
