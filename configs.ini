[DEFAULT]
debug = False
seed = 0
output_dir = 'output'
run_name = 'temp_run'
git_sha1 = 'unknown'
git_diff = ''

[training]
project = 'unspecified'
resume_from_checkpoint = False
resume_wandb_runid = None
max_length = 2048
filter_out_shorts = False
use_default_num_items_getter = True
use_eagle_pipeline = False
model_init_ckpt = None
sequential_loading = False

overwrite_output_dir = True
save_strategy = "steps"
save_steps = 500
save_total_limit = 2
optim = "adamw_torch_fused"
# same as Hydra++
num_train_epochs = 10
max_steps = -1
per_device_train_batch_size = 2
gradient_accumulation_steps = 3
average_tokens_across_devices = False
warmup_steps = 2000
#warmup_steps = 300
lr_scheduler_type = "constant_with_warmup"
learning_rate = 3e-5
ddp_find_unused_parameters = False
max_grad_norm = 0.5
adam_beta1 = 0.9
adam_beta2 = 0.95
logging_steps = 25
logging_first_step = True
bf16 = True
tf32 = False
report_to = "none"
#eval_strategy = "no"
eval_strategy = "steps"
per_device_eval_batch_size = 1
eval_steps = 100
# Always set dataloader_drop_last to avoid empty batch for some node when world > 1.
dataloader_drop_last = True
# in order for dataloader_drop_last to be effective, avoid using streaming load_dataset:
# See transformers/trainer.py:get_train_dataloader() and
# https://huggingface.co/docs/datasets/en/about_mapstyle_vs_iterable
ddp_backend = "gloo"
#ddp_backend = "nccl"
deepspeed = None

[dataset]
#manual_sample_ids = ['v4PzAY8_0', 'oM7QCY2_0', 'efVCaLN_0']
manual_sample_ids = []
read_eagle_format = False
max_read_items = None
path = "output/datasets/default"
eval_path = None

[dataset_generation]
save_every = 1000
batch_size = 1
#debug_target = v4PzAY8_0
debug_target = None
ds_prefix = 'ds_'
max_length = 2048
sharegpt_path = 'Aeala/ShareGPT_Vicuna_unfiltered'

[modeling]
tokenizer_init = ""
dtype = 'torch.bfloat16'
max_memory = None
free_base_layers = False
model_path = None
draft_config_modify = dict()
draft_config_custom_modify = dict()
stand_alone_draft_model_path = None
stand_alone_draft_model_key_adapt = None
stand_alone_draft_model_key_modify: dict(
        yuhuili = [
            (r"^fc\.", "eagle_fc."),
            ("embed_tokens.weight", None),
        ]
    )

[inference]
max_new_tokens = 2048
timer = False
dynamic_draft_max_depth = 6
dynamic_draft_top_k = 10
dynamic_draft_all_top_k = 59

[modeling.@llama2_7b]
tokenizer_path = "meta-llama/Llama-2-7b-chat-hf"
chat_template = "models/speculative_llama/chat_template.jinja2"
tokenizer_add_tokens = "dict(pad_token=tokenizer.eos_token)"

model_path = "meta-llama/Llama-2-7b-chat-hf"

[modeling.@llama2_7b_base_and_llama2_7b_drafter_using_eagle2]
tokenizer_path = "meta-llama/Llama-2-7b-chat-hf"
chat_template = "models/speculative_llama/chat_template.jinja2"
tokenizer_add_tokens = "dict(pad_token=tokenizer.eos_token)"

init_base_model = ('SpeculativeLlamaForCausalLM', 'meta-llama/Llama-2-7b-chat-hf')
init_draft_config = ('LlamaDrafter', 'meta-llama/Llama-2-7b-chat-hf')
init_speculative_algorithm = ('EagleV2', 'dict(draft_layers=1)')
free_base_layers = 'num_hidden_layers'

[modeling.@llama2_7b_base_and_qwen3_moe_30A3B_drafter_using_eagle2]
tokenizer_path = "meta-llama/Llama-2-7b-chat-hf"
chat_template = "models/speculative_llama/chat_template.jinja2"
tokenizer_add_tokens = "dict(pad_token=tokenizer.eos_token)"

init_base_model = ('SpeculativeLlamaForCausalLM', 'meta-llama/Llama-2-7b-chat-hf')
init_draft_config = ('Qwen3MoeDrafter', 'Qwen/Qwen3-30B-A3B-Instruct-2507')
init_speculative_algorithm = ('EagleV2', 'dict(draft_layers=1)')
free_base_layers = 'num_hidden_layers'

[modeling.@qwen3_moe_30A3B]
tokenizer_path = "Qwen/Qwen3-30B-A3B-Instruct-2507"
chat_template = "models/speculative_qwen3_moe/chat_template.jinja2"
tokenizer_add_tokens = "dict(unk_token=tokenizer.eos_token)"

model_path = "Qwen/Qwen3-30B-A3B-Instruct-2507"

[modeling.@qwen3_moe_30A3B_base_and_qwen3_moe_30A3B_drafter_using_eagle2]
tokenizer_path = "Qwen/Qwen3-30B-A3B-Instruct-2507"
chat_template = "models/speculative_qwen3_moe/chat_template.jinja2"
tokenizer_add_tokens = "dict(unk_token=tokenizer.eos_token)"

init_base_model = ('SpeculativeQwen3MoeForCausalLM', 'Qwen/Qwen3-30B-A3B-Instruct-2507')
init_draft_config = ('Qwen3MoeDrafter', 'Qwen/Qwen3-30B-A3B-Instruct-2507')
init_speculative_algorithm = ('EagleV2', 'dict(draft_layers=1)')
free_base_layers = 'num_hidden_layers'

[modeling.@qwen3_moe_30A3B_base_and_qwen3_4B_drafter_using_eagle2]
tokenizer_path = "Qwen/Qwen3-30B-A3B-Instruct-2507"
chat_template = "models/speculative_qwen3_moe/chat_template.jinja2"
tokenizer_add_tokens = "dict(unk_token=tokenizer.eos_token)"

init_base_model = ('SpeculativeQwen3MoeForCausalLM', 'Qwen/Qwen3-30B-A3B-Instruct-2507')
init_draft_config = ('Qwen3Drafter', 'Qwen/Qwen3-4B-Instruct-2507')
init_speculative_algorithm = ('EagleV2', 'dict(draft_layers=1)')
free_base_layers = 'num_hidden_layers'

draft_config_modify: dict(
        intermediate_size="base.moe_intermediate_size * base.num_experts_per_tok",
        num_key_value_heads="base.num_key_value_heads",
        rope_theta="base.rope_theta",
    )

[modeling.@qwen3_4B]
tokenizer_path = "Qwen/Qwen3-4B-Instruct-2507"
chat_template = "models/speculative_qwen3/chat_template.jinja2"
tokenizer_add_tokens = "dict(unk_token=tokenizer.eos_token)"

model_path = "Qwen/Qwen3-4B-Instruct-2507"

[modeling.@qwen3_4B_base_and_qwen3_4B_drafter_using_eagle2]
tokenizer_path = "Qwen/Qwen3-4B-Instruct-2507"
chat_template = "models/speculative_qwen3/chat_template.jinja2"
tokenizer_add_tokens = "dict(unk_token=tokenizer.eos_token)"

init_base_model = ('SpeculativeQwen3ForCausalLM', 'Qwen/Qwen3-4B-Instruct-2507')
init_draft_config = ('Qwen3Drafter', 'Qwen/Qwen3-4B-Instruct-2507')
init_speculative_algorithm = ('EagleV2', 'dict(draft_layers=1)')
free_base_layers = 'num_hidden_layers'

[modeling.@qwen3_4B_base_and_qwen3_moe_30A3B_drafter_using_eagle2]
tokenizer_path = "Qwen/Qwen3-4B-Instruct-2507"
chat_template = "models/speculative_qwen3/chat_template.jinja2"
tokenizer_add_tokens = "dict(unk_token=tokenizer.eos_token)"

init_base_model = ('SpeculativeQwen3ForCausalLM', 'Qwen/Qwen3-4B-Instruct-2507')
init_draft_config = ('Qwen3MoeDrafter', 'Qwen/Qwen3-30B-A3B-Instruct-2507')
init_speculative_algorithm = ('EagleV2', 'dict(draft_layers=1)')
free_base_layers = 'num_hidden_layers'

draft_config_modify: dict(
        moe_intermediate_size="base.intermediate_size // draft_config.num_experts_per_tok",
        num_key_value_heads="base.num_key_value_heads",
        rope_theta="base.rope_theta",
    )

############# customized MoE models ################
[modeling.@qwen3_4B_base_and_customized_qwen3_moe_30A3B_drafter_using_eagle2]
tokenizer_path = "Qwen/Qwen3-4B-Instruct-2507"
chat_template = "models/speculative_qwen3/chat_template.jinja2"
tokenizer_add_tokens = "dict(unk_token=tokenizer.eos_token)"

init_base_model = ('SpeculativeQwen3ForCausalLM', 'Qwen/Qwen3-4B-Instruct-2507')
init_draft_config = ('Qwen3MoeDrafter', 'Qwen/Qwen3-30B-A3B-Instruct-2507')
init_speculative_algorithm = ('EagleV2', 'dict(draft_layers=1)')
free_base_layers = 'num_hidden_layers'

## if number of activate experts (topK) doubled, consider cut router_aux_loss_coef in half!
draft_config_modify: dict(
        moe_intermediate_size="base.intermediate_size // draft_config.num_experts_per_tok",
        num_key_value_heads="base.num_key_value_heads",
        rope_theta="base.rope_theta",
        zero_compute_experts="32",
        shared_experts="2",
        router_aux_loss_coef="draft_config.router_aux_loss_coef / 2"
    )
